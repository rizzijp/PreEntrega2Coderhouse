{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "574bca8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSAL cache backend → Keychain (macOS) signal=/Users/jprizzi/.mailreader_msal_signal\n",
      "Aquí tienes el análisis de tus correos:\n",
      "\n",
      "**1) Panorama (Tendencias y Temas):**\n",
      "*   Gestión financiera y contable (autónomos, proveedores, imputaciones, informes).\n",
      "*   Coordinación de tareas contables y revisión de saldos con [REDACTED].\n",
      "*   Novedades tecnológicas (GPT-5 en Microsoft 365 Copilot).\n",
      "*   Asuntos legales y de RRHH (procedimiento por fallecimiento de empleado).\n",
      "*   Solicitud y envío de documentación impositiva.\n",
      "*   Gestión de inversiones (venta de bonos argentinos).\n",
      "*   Comunicación y seguimiento de tareas pendientes.\n",
      "\n",
      "**2) Top Prioridades con Acción Sugerida:**\n",
      "1.  **AUTONOMOS 08-2025 (Vence 08-9-2025):** Verificar débito automático o usar VEP enviado por [REDACTED].\n",
      "2.  **Proveedores/Contabilidad:** Coordinar con [REDACTED] la revisión de imputaciones y actualización de sistemas.\n",
      "3.  **Fallecimiento Colaborador:** Revisar procedimiento legal y contable para el empleado fallecido.\n",
      "4.  **Operación Bonos:** [REDACTED] debe confirmar la venta de la totalidad de la posición de bonos argentinos.\n",
      "\n",
      "**3) Fechas/Calendario Detectados:**\n",
      "*   **2025-09-08:** Vencimiento de Autónomos 08-2025.\n",
      "\n",
      "**4) Adjuntos Relevantes (Asunto):**\n",
      "*   AUTONOMOS 08-2025 VENCE 08-9-2025 (contiene VEP).\n",
      "*   RE: SOLICITUD DE CMO5 (contiene legajo impositivo actualizado).\n",
      "\n",
      "**5) KPIs:**\n",
      "*   Total de correos: 8\n",
      "*   Correos sin leer: 1\n",
      "*   Correos con adjuntos: 2\n",
      "*   Correos respondidos: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'gemini-2.5-flash',\n",
       " 'saved_to': None,\n",
       " 'usage': {'prompt_tokens': 4442,\n",
       "  'response_tokens': 441,\n",
       "  'thoughts_tokens': 1103,\n",
       "  'total_tokens': 5986},\n",
       " 'rows_in': 8,\n",
       " 'rows_after_clamp': 8}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types  # <- para GenerateContentConfig\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "load_dotenv()\n",
    "\n",
    "# En el SDK nuevo no hace falta el prefijo \"models/\"\n",
    "# Si lo traés así desde el entorno, se lo quitamos para evitar 404.\n",
    "_MODEL_ENV = os.getenv(\"GOOGLE_GEMINI_MODEL\", \"gemini-2.5-flash\")\n",
    "MODEL = _MODEL_ENV.replace(\"models/\", \"\")\n",
    "# El cliente toma GEMINI_API_KEY o GOOGLE_API_KEY desde el entorno\n",
    "client = genai.Client()\n",
    "\n",
    "# === Config única (editar solo acá) ===\n",
    "CONFIG = {\n",
    "    \"from_mailreader\": True,\n",
    "    \"limit\": 200, #limite de mails a buscar\n",
    "    \"days_back\": 30, #cantidad de dias hacia atras para buscar mails\n",
    "    \"json_path\": None,\n",
    "    \"max_chars\": 120_000,\n",
    "    \"out\": None,                 # \"respuesta.txt\" o None\n",
    "    \"model\": MODEL,              # o \"gemini-2.5-pro\", etc.\n",
    "    \"temperature\": None,         # ej. 0.3\n",
    "    \"max_output_tokens\": None,   # ej. 1200\n",
    "    \"response_mime_type\": None,  # ej. \"text/markdown\"\n",
    "    \"debug\": False,\n",
    "}\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "\n",
    "def load_rows_from_json(path: str) -> list[dict]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    if isinstance(data, dict) and \"rows\" in data:\n",
    "        return data[\"rows\"]\n",
    "    if not isinstance(data, list):\n",
    "        raise ValueError(\"El JSON no es una lista de correos.\")\n",
    "    return data\n",
    "\n",
    "def load_rows_from_mailreader(limit: int | None = None, days_back: int | None = None) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Importa mailreader13 y obtiene los correos en memoria.\n",
    "    Cambiá 'mailreader13' si tu módulo se llama diferente.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import mailreader13_beta as mailreader\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"No pude importar mailreader13: {e}\")\n",
    "\n",
    "    if hasattr(mailreader, \"collect_rows\"):\n",
    "        # Pasamos limit/days_back si tu collect_rows los soporta\n",
    "        try:\n",
    "            rows = mailreader.collect_rows(limit=limit, days_back=days_back or None, debug=False)\n",
    "        except TypeError:\n",
    "            # fallback por si tu firma solo acepta limit\n",
    "            rows = mailreader.collect_rows(limit=limit)\n",
    "    elif hasattr(mailreader, \"main\"):\n",
    "        out_path = os.getenv(\"OUTPUT_JSON\", \"emails.json\")\n",
    "        mailreader.main()\n",
    "        rows = load_rows_from_json(out_path)\n",
    "    else:\n",
    "        raise RuntimeError(\"mailreader no expone collect_rows() ni main().\")\n",
    "\n",
    "    return rows\n",
    "\n",
    "def clamp_rows(rows: list[dict], max_chars: int = 120_000) -> list[dict]:\n",
    "    \"\"\"Devuelve filas *compactas* y recorta por caracteres de forma consistente.\"\"\"\n",
    "    compact_rows, total = [], 0\n",
    "    for r in rows:\n",
    "        compact = {\n",
    "            \"Fecha\": r.get(\"Fecha\"),\n",
    "            \"ReceivedUTC\": r.get(\"ReceivedUTC\"),\n",
    "            \"Remitente\": r.get(\"Remitente\"),\n",
    "            \"RemitenteNombre\": r.get(\"RemitenteNombre\"),\n",
    "            \"Asunto\": r.get(\"Asunto\"),\n",
    "            \"Cuerpo\": (r.get(\"Cuerpo\") or \"\")[:2000],  # límite duro por mail\n",
    "            \"TieneAdjuntos\": bool(r.get(\"TieneAdjuntos\")),\n",
    "            \"Leido\": bool(r.get(\"Leido\")),\n",
    "            \"Respondido\": bool(r.get(\"Respondido\")),\n",
    "            \"Categorias\": r.get(\"Categorias\"),\n",
    "            \"WebLink\": r.get(\"WebLink\"),\n",
    "        }\n",
    "        s = json.dumps(compact, ensure_ascii=False)\n",
    "        if total + len(s) > max_chars and compact_rows:\n",
    "            break\n",
    "        compact_rows.append(compact)\n",
    "        total += len(s)\n",
    "    return compact_rows\n",
    "\n",
    "def build_prompt(rows: list[dict]) -> tuple[str, list[str]]:\n",
    "    \"\"\"\n",
    "    Devuelve (system_instruction, contents_list) en el formato esperado por google-genai.\n",
    "    - system_instruction: string con las reglas\n",
    "    - contents_list: lista[str] con el payload (el SDK la convierte a UserContent)\n",
    "    \"\"\"\n",
    "    system_instruction = (\n",
    "        \"Eres un asistente que analiza correos de Outlook. \"\n",
    "        \"Responde en español, claro y estructurado, en menos de 200 palabras.\"\n",
    "        \"IMPORTANTE: Oculta datos sensibles reemplazando los nombres de personas por [REDACTED] y las direcciones de correo electronico por [MAIL]. \"\n",
    "        \"No inventes ni modifiques información, solo redacta lo sensible. Entrega:\\n\"\n",
    "        \"1) Panorama (tendencias y temas) en 5-8 bullets.\\n\"\n",
    "        \"2) Top prioridades (máx 10) con acción sugerida.\\n\"\n",
    "        \"3) Fechas/calendario detectados (si se mencionan).\\n\"\n",
    "        \"4) Adjuntos relevantes (asunto).\\n\"\n",
    "        \"5) KPIs: total, sin leer, con adjuntos, respondidos.\\n\"\n",
    "        f\"Fecha de generación: {datetime.now().isoformat(timespec='seconds')}.\"\n",
    "    )\n",
    "\n",
    "    contents_list = [\n",
    "        \"A continuación va la lista JSON de correos.\",\n",
    "        json.dumps(rows, ensure_ascii=False),\n",
    "    ]\n",
    "    return system_instruction, contents_list\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Ejecutor\n",
    "# =========================\n",
    "def run_analysis(config: dict):\n",
    "    \"\"\"Ejecuta el análisis leyendo TODO de config.\"\"\"\n",
    "    from_mailreader     = config[\"from_mailreader\"]\n",
    "    limit               = config[\"limit\"]\n",
    "    days_back           = config[\"days_back\"]\n",
    "    json_path           = config[\"json_path\"]\n",
    "    max_chars           = config[\"max_chars\"]\n",
    "    out                 = config[\"out\"]\n",
    "    model               = (config[\"model\"] or MODEL).replace(\"models/\", \"\")\n",
    "    temperature         = config[\"temperature\"]\n",
    "    max_output_tokens   = config[\"max_output_tokens\"]\n",
    "    response_mime_type  = config[\"response_mime_type\"]\n",
    "    debug               = config[\"debug\"]\n",
    "\n",
    "    # 1) Carga de correos\n",
    "    if from_mailreader:\n",
    "        rows = load_rows_from_mailreader(limit=limit, days_back=days_back)\n",
    "    else:\n",
    "        jp = json_path or os.getenv(\"OUTPUT_JSON\", \"emails.json\")\n",
    "        rows = load_rows_from_json(jp)\n",
    "\n",
    "    if not rows:\n",
    "        return \"\", {\"error\": \"No hay correos para analizar.\"}\n",
    "\n",
    "    # 2) Orden + recorte\n",
    "    rows_sorted  = sorted(rows, key=lambda r: r.get(\"ReceivedUTC\") or r.get(\"Fecha\") or \"\", reverse=True)\n",
    "    rows_compact = clamp_rows(rows_sorted, max_chars=max_chars)\n",
    "\n",
    "    # 3) Prompt\n",
    "    system_instruction, contents = build_prompt(rows_compact)\n",
    "\n",
    "    # 4) Llamada al modelo\n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=contents,\n",
    "            config=types.GenerateContentConfig(\n",
    "                system_instruction=system_instruction,\n",
    "                max_output_tokens=max_output_tokens if max_output_tokens else None,\n",
    "                temperature=temperature if temperature else None,\n",
    "                response_mime_type=response_mime_type if response_mime_type else None,\n",
    "            ),\n",
    "        )\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            raise\n",
    "        return \"\", {\"error\": f\"Fallo generate_content: {e}\", \"model\": model}\n",
    "\n",
    "    text = getattr(response, \"text\", \"\") or \"\"\n",
    "\n",
    "    # 5) Guardado opcional\n",
    "    saved_path = None\n",
    "    if out:\n",
    "        saved_path = os.path.abspath(out)\n",
    "        with open(out, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "\n",
    "    # 6) Métricas de uso\n",
    "    usage = {}\n",
    "    if hasattr(response, \"usage_metadata\") and response.usage_metadata:\n",
    "        u = response.usage_metadata\n",
    "        usage = {\n",
    "            \"prompt_tokens\": getattr(u, \"prompt_token_count\", None),\n",
    "            \"response_tokens\": getattr(u, \"candidates_token_count\", None),\n",
    "            \"thoughts_tokens\": getattr(u, \"thoughts_token_count\", None),\n",
    "            \"total_tokens\": getattr(u, \"total_token_count\", None),\n",
    "        }\n",
    "\n",
    "    meta = {\n",
    "        \"model\": model,\n",
    "        \"saved_to\": saved_path,\n",
    "        \"usage\": usage,\n",
    "        \"rows_in\": len(rows),\n",
    "        \"rows_after_clamp\": len(rows_compact),\n",
    "    }\n",
    "    return text, meta\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Invocación\n",
    "# =========================\n",
    "texto, meta = run_analysis(CONFIG)\n",
    "print(texto)\n",
    "meta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
