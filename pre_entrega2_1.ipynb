{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2e37ef6",
   "metadata": {},
   "source": [
    "# Proyecto: Informe de Emails Recibidos\n",
    "\n",
    "## Introducción\n",
    "\n",
    "### Problema a abordar\n",
    "\n",
    "La problemática radica en la gran cantidad de mails que recibimos todos los días. En esa marea de mails, se suelen pasar por alto mails importantes. Muchas veces, un mail queda perdido en el buzón de entrada y luego se hace difícil encontrarlo. Si además se refería a algo urgente que necesitaba verse a tiempo, se torna un problema mayor.\n",
    "\n",
    "### Desarrollo de la propuesta de solución\n",
    "\n",
    "La propuesta es generar un informe de envío semanal a una casilla de correo que haga un repaso de todos los mails recibidos en la última semana, de manera que nos permita ponernos al día con los asuntos a atender. Se buscará que el informe ordene los temas relevados por categorías según los mails recibidos.\n",
    "La idea consiste en acceder a un mail en Outlook o exportar los mails de la última semana y a través de un prompt leer los mails en cuestión y armar un resumen de estos ordenándolos por categoría y prioridad.\n",
    "\n",
    "**Prompt a probar:** Analizar todos los emails de la última semana (remitente, asunto, contenido y adjuntos) y generar un informe agrupando por categoría. Identificar los mails de mayor prioridad y proponer acciones a llevar a cabo en la semana.\n",
    "\n",
    "**Prompt más estructurado:**\n",
    "\n",
    "```text\n",
    "Eres un asistente que analiza correos de Outlook.  \n",
    "Responde en español, claro y estructurado, en menos de 200 palabras.  \n",
    "\n",
    "IMPORTANTE: Oculta datos sensibles reemplazando los nombres de personas por [REDACTED] y las direcciones de correo electrónico por [MAIL].  \n",
    "No inventes ni modifiques información, solo redacta lo sensible.  \n",
    "\n",
    "Entrega:\n",
    "1) Panorama (tendencias y temas) en 5-8 bullets.  \n",
    "2) Top prioridades (máx 10) con acción sugerida.  \n",
    "3) Fechas/calendario detectados (si se mencionan).  \n",
    "4) Adjuntos relevantes (asunto).  \n",
    "5) KPIs: total, sin leer, con adjuntos, respondidos.  \n",
    "\n",
    "```\n",
    "\n",
    "Para usar también el modelo texto a imagen, buscaremos que el informe comience con un tablero que muestre toda la información resumida de manera muy visual.\n",
    "\n",
    "\n",
    "### Viabilidad del proyecto\n",
    "El proyecto lo podemos dividir en 3 etapas:\n",
    "\n",
    "#### 1) Obtener mails\n",
    "\n",
    "Primero necesitamos tener el contenido de los correos electrónicos recibidos en una casilla de correo. Para esto, se buscará acceder con la API de OpenAI a la bandeja de entrada de Outlook o automatizar la exportación de los mails de la última semana para luego poder entregarle el archivo a OpenAI. Esta etapa es la que a priori resulta la más desafiante con los conocimientos que poseo, pero utilizando ChatGPT es viable encontrar una manera de realizarlo.\n",
    "\n",
    "#### 2) Generar informe\n",
    "\n",
    "La segunda etapa sería cuando entra en juego los modelos de ChatGPT y Dall-E para analizar todos los mails y armar el informe. En esta etapa, se necesitará pulir el prompt en ambos modelos para lograr el objetivo.\n",
    "\n",
    "#### 3) Enviar informe\n",
    "\n",
    "La tercera y última etapa consiste en enviar por email el informe generado. Por más que actualmente no cuento con los conocimientos para realizarlo, utilizando ChatGPT es viable encontrar como se puede llevar a cabo.\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "Sistematizar la lectura de correos para transformar información dispersa en un informe ejecutivo claro y accionable.\n",
    "\n",
    "Estandarizar criterios de priorización y seguimiento para mejorar la toma de decisiones.\n",
    "\n",
    "Proteger la privacidad mediante anonimización constante de datos personales.\n",
    "\n",
    "## Metodología\n",
    "\n",
    "El flujo implementado debe convertir un alto volumen de correos en un producto informativo breve, verificable y anonimizado, apto para lectura ejecutiva y para integrar a tableros/automatizaciones.\n",
    "\n",
    "Las 3 etapas mencionadas se llevan a cabo de la siguiente manera:\n",
    "\n",
    "**Etapa 1: Obtener mails (Recolección y depuración)**\n",
    "\n",
    "- Obtengo correos desde Microsoft Graph (Outlook) del período de análisis y se eliminan duplicidades, ruidos y datos superfluos, preservando la trazabilidad.\n",
    "- Normalizo campos (remitente, asunto, cuerpo, fechas, flags/“Destacado”, adjuntos, webLink). Excluyo correos de promociones, newsletters y enviados por mí salvo que sean relevantes (respuestas a clientes).\n",
    "\n",
    "**Etapa 2: Generar informe (Diseño de prompt y orquestación)**\n",
    "\n",
    "- Uso instrucciones de sistema claras (rol, estilo, longitud máxima, idioma) y secciones obligatorias (Panorama, Top prioridades con acción, Fechas, Adjuntos, KPIs) para producir un resumen ejecutivo con secciones fijas de menos de 200 palabras.\n",
    "- Aplico anonimización para reemplazar emails y nombres propios por placeholders antes de enviar al modelo y también en la salida. \n",
    "- Se verifican indicadores básicos (totales, no leídos, adjuntos, respondidos) para asegurar calidad.\n",
    "\n",
    "**Etapa 3: Enviar Informe**\n",
    "\n",
    "- Genero PDF profesional del informe (prioridades, fechas, adjuntos).\n",
    "- Automatización y monitoreo: Ejecución programada (Power Automate).\n",
    "\n",
    "## Herramientas y tecnologías\n",
    "\n",
    "Se utilizan:\n",
    "- Visual Studio Code\n",
    "- Jupyter Notebook\n",
    "- Github\n",
    "\n",
    "- Lenguaje: **Python**\n",
    "- Modelo AI: **Google Gemini**\n",
    "\n",
    "**Tecnica de Prompt:**\n",
    "\n",
    "Se busca utilizar en primera instancia la tecnica zero-shot dado que la cantidad de tokens va a ser alta de por si por la cantidad de correos que tiene que analizar el modelo. Se trata de optimizar el consumo de tokens. De igual manera, se evaluará si el resultado obtenido es acorde a lo esperado y, en caso de no serlo, se optará por agregar ejemplos (few-shot) para obtener respuestas breves y bien formateadas.\n",
    "\n",
    "## Implementación\n",
    "\n",
    "A continuación se incluye el código para llegar a la solución propuesta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "574bca8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aquí tienes el análisis de tus correos:\n",
      "\n",
      "**1) Panorama (tendencias y temas):**\n",
      "*   Gestión de pagos y vencimientos de autónomos.\n",
      "*   Actualizaciones y seguimiento de tareas contables y financieras.\n",
      "*   Comunicación y agradecimiento por soporte en gestión de proveedores.\n",
      "*   Noticias sobre nuevas herramientas tecnológicas (GPT-5 en Copilot).\n",
      "*   Asesoramiento legal sobre gestión de fallecimiento de empleados.\n",
      "*   Solicitud y envío de documentación impositiva.\n",
      "*   Confirmación de operaciones financieras (venta de bonos).\n",
      "\n",
      "**2) Top prioridades con acción sugerida:**\n",
      "1.  **Pago Autónomos 08-2025:** Verificar débito o realizar pago del VEP (Vence 08-09-2025).\n",
      "2.  **Gestión de Proveedores:** Seguir la actualización del sistema de pendientes por [REDACTED].\n",
      "3.  **Revisiones Contables:** Confirmar con [REDACTED] estado de cuenta USD Allaria y coordinar con [REDACTED] por asientos mensuales.\n",
      "4.  **Asuntos de Dividendos:** Consultar con [REDACTED] por revisión de planillas y actas.\n",
      "5.  **Documentación Impositiva:** Revisar y archivar legajo enviado por [REDACTED].\n",
      "6.  **Operación de Bonos:** Confirmar con [REDACTED] la venta de bonos.\n",
      "\n",
      "**3) Fechas/calendario detectados:**\n",
      "*   **08-09-2025:** Vencimiento pago de autónomos 08-2025.\n",
      "\n",
      "**4) Adjuntos relevantes (asunto):**\n",
      "*   AUTONOMOS 08-2025 VENCE 08-9-2025 (VEP)\n",
      "*   RE: SOLICITUD DE CMO5 (Legajo impositivo actualizado)\n",
      "\n",
      "**5) KPIs:**\n",
      "*   Total de correos: 8\n",
      "*   Correos sin leer: 1\n",
      "*   Correos con adjuntos: 2\n",
      "*   Correos respondidos: 2\n",
      "\n",
      "Fecha de generación: 2025-09-01T15:54:47\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'gemini-2.5-flash',\n",
       " 'saved_to': None,\n",
       " 'usage': {'prompt_tokens': 4442,\n",
       "  'response_tokens': 492,\n",
       "  'thoughts_tokens': 2189,\n",
       "  'total_tokens': 7123},\n",
       " 'rows_in': 8,\n",
       " 'rows_after_clamp': 8}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types  # <- para GenerateContentConfig\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "load_dotenv()\n",
    "\n",
    "# En el SDK nuevo no hace falta el prefijo \"models/\"\n",
    "# Si lo traés así desde el entorno, se lo quitamos para evitar 404.\n",
    "_MODEL_ENV = os.getenv(\"GOOGLE_GEMINI_MODEL\", \"gemini-2.5-flash\")\n",
    "MODEL = _MODEL_ENV.replace(\"models/\", \"\")\n",
    "# El cliente toma GEMINI_API_KEY o GOOGLE_API_KEY desde el entorno\n",
    "client = genai.Client()\n",
    "\n",
    "# === Config única (editar solo acá) ===\n",
    "CONFIG = {\n",
    "    \"from_mailreader\": True,\n",
    "    \"limit\": 200, #limite de mails a buscar\n",
    "    \"days_back\": 30, #cantidad de dias hacia atras para buscar mails\n",
    "    \"json_path\": None,\n",
    "    \"max_chars\": 120_000,\n",
    "    \"out\": None,                 # \"respuesta.txt\" o None\n",
    "    \"model\": MODEL,              # o \"gemini-2.5-pro\", etc.\n",
    "    \"temperature\": None,         # ej. 0.3\n",
    "    \"max_output_tokens\": None,   # ej. 1200\n",
    "    \"response_mime_type\": None,  # ej. \"text/markdown\"\n",
    "    \"debug\": False,\n",
    "}\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "\n",
    "def load_rows_from_json(path: str) -> list[dict]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    if isinstance(data, dict) and \"rows\" in data:\n",
    "        return data[\"rows\"]\n",
    "    if not isinstance(data, list):\n",
    "        raise ValueError(\"El JSON no es una lista de correos.\")\n",
    "    return data\n",
    "\n",
    "def load_rows_from_mailreader(limit: int | None = None, days_back: int | None = None) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Importa mailreader13 y obtiene los correos en memoria.\n",
    "    Cambiá 'mailreader13' si tu módulo se llama diferente.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import mailreader13_beta as mailreader\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"No pude importar mailreader13: {e}\")\n",
    "\n",
    "    if hasattr(mailreader, \"collect_rows\"):\n",
    "        # Pasamos limit/days_back si tu collect_rows los soporta\n",
    "        try:\n",
    "            rows = mailreader.collect_rows(limit=limit, days_back=days_back or None, debug=False)\n",
    "        except TypeError:\n",
    "            # fallback por si tu firma solo acepta limit\n",
    "            rows = mailreader.collect_rows(limit=limit)\n",
    "    elif hasattr(mailreader, \"main\"):\n",
    "        out_path = os.getenv(\"OUTPUT_JSON\", \"emails.json\")\n",
    "        mailreader.main()\n",
    "        rows = load_rows_from_json(out_path)\n",
    "    else:\n",
    "        raise RuntimeError(\"mailreader no expone collect_rows() ni main().\")\n",
    "\n",
    "    return rows\n",
    "\n",
    "def clamp_rows(rows: list[dict], max_chars: int = 120_000) -> list[dict]:\n",
    "    \"\"\"Devuelve filas *compactas* y recorta por caracteres de forma consistente.\"\"\"\n",
    "    compact_rows, total = [], 0\n",
    "    for r in rows:\n",
    "        compact = {\n",
    "            \"Fecha\": r.get(\"Fecha\"),\n",
    "            \"ReceivedUTC\": r.get(\"ReceivedUTC\"),\n",
    "            \"Remitente\": r.get(\"Remitente\"),\n",
    "            \"RemitenteNombre\": r.get(\"RemitenteNombre\"),\n",
    "            \"Asunto\": r.get(\"Asunto\"),\n",
    "            \"Cuerpo\": (r.get(\"Cuerpo\") or \"\")[:2000],  # límite duro por mail\n",
    "            \"TieneAdjuntos\": bool(r.get(\"TieneAdjuntos\")),\n",
    "            \"Leido\": bool(r.get(\"Leido\")),\n",
    "            \"Respondido\": bool(r.get(\"Respondido\")),\n",
    "            \"Categorias\": r.get(\"Categorias\"),\n",
    "            \"WebLink\": r.get(\"WebLink\"),\n",
    "        }\n",
    "        s = json.dumps(compact, ensure_ascii=False)\n",
    "        if total + len(s) > max_chars and compact_rows:\n",
    "            break\n",
    "        compact_rows.append(compact)\n",
    "        total += len(s)\n",
    "    return compact_rows\n",
    "\n",
    "def build_prompt(rows: list[dict]) -> tuple[str, list[str]]:\n",
    "    \"\"\"\n",
    "    Devuelve (system_instruction, contents_list) en el formato esperado por google-genai.\n",
    "    - system_instruction: string con las reglas\n",
    "    - contents_list: lista[str] con el payload (el SDK la convierte a UserContent)\n",
    "    \"\"\"\n",
    "    system_instruction = (\n",
    "        \"Eres un asistente que analiza correos de Outlook. \"\n",
    "        \"Responde en español, claro y estructurado, en menos de 200 palabras.\"\n",
    "        \"IMPORTANTE: Oculta datos sensibles reemplazando los nombres de personas por [REDACTED] y las direcciones de correo electronico por [MAIL]. \"\n",
    "        \"No inventes ni modifiques información, solo redacta lo sensible. Entrega:\\n\"\n",
    "        \"1) Panorama (tendencias y temas) en 5-8 bullets.\\n\"\n",
    "        \"2) Top prioridades (máx 10) con acción sugerida.\\n\"\n",
    "        \"3) Fechas/calendario detectados (si se mencionan).\\n\"\n",
    "        \"4) Adjuntos relevantes (asunto).\\n\"\n",
    "        \"5) KPIs: total, sin leer, con adjuntos, respondidos.\\n\"\n",
    "        f\"Fecha de generación: {datetime.now().isoformat(timespec='seconds')}.\"\n",
    "    )\n",
    "\n",
    "    contents_list = [\n",
    "        \"A continuación va la lista JSON de correos.\",\n",
    "        json.dumps(rows, ensure_ascii=False),\n",
    "    ]\n",
    "    return system_instruction, contents_list\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Ejecutor\n",
    "# =========================\n",
    "def run_analysis(config: dict):\n",
    "    \"\"\"Ejecuta el análisis leyendo TODO de config.\"\"\"\n",
    "    from_mailreader     = config[\"from_mailreader\"]\n",
    "    limit               = config[\"limit\"]\n",
    "    days_back           = config[\"days_back\"]\n",
    "    json_path           = config[\"json_path\"]\n",
    "    max_chars           = config[\"max_chars\"]\n",
    "    out                 = config[\"out\"]\n",
    "    model               = (config[\"model\"] or MODEL).replace(\"models/\", \"\")\n",
    "    temperature         = config[\"temperature\"]\n",
    "    max_output_tokens   = config[\"max_output_tokens\"]\n",
    "    response_mime_type  = config[\"response_mime_type\"]\n",
    "    debug               = config[\"debug\"]\n",
    "\n",
    "    # 1) Carga de correos\n",
    "    if from_mailreader:\n",
    "        rows = load_rows_from_mailreader(limit=limit, days_back=days_back)\n",
    "    else:\n",
    "        jp = json_path or os.getenv(\"OUTPUT_JSON\", \"emails.json\")\n",
    "        rows = load_rows_from_json(jp)\n",
    "\n",
    "    if not rows:\n",
    "        return \"\", {\"error\": \"No hay correos para analizar.\"}\n",
    "\n",
    "    # 2) Orden + recorte\n",
    "    rows_sorted  = sorted(rows, key=lambda r: r.get(\"ReceivedUTC\") or r.get(\"Fecha\") or \"\", reverse=True)\n",
    "    rows_compact = clamp_rows(rows_sorted, max_chars=max_chars)\n",
    "\n",
    "    # 3) Prompt\n",
    "    system_instruction, contents = build_prompt(rows_compact)\n",
    "\n",
    "    # 4) Llamada al modelo\n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=contents,\n",
    "            config=types.GenerateContentConfig(\n",
    "                system_instruction=system_instruction,\n",
    "                max_output_tokens=max_output_tokens if max_output_tokens else None,\n",
    "                temperature=temperature if temperature else None,\n",
    "                response_mime_type=response_mime_type if response_mime_type else None,\n",
    "            ),\n",
    "        )\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            raise\n",
    "        return \"\", {\"error\": f\"Fallo generate_content: {e}\", \"model\": model}\n",
    "\n",
    "    text = getattr(response, \"text\", \"\") or \"\"\n",
    "\n",
    "    # 5) Guardado opcional\n",
    "    saved_path = None\n",
    "    if out:\n",
    "        saved_path = os.path.abspath(out)\n",
    "        with open(out, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "\n",
    "    # 6) Métricas de uso\n",
    "    usage = {}\n",
    "    if hasattr(response, \"usage_metadata\") and response.usage_metadata:\n",
    "        u = response.usage_metadata\n",
    "        usage = {\n",
    "            \"prompt_tokens\": getattr(u, \"prompt_token_count\", None),\n",
    "            \"response_tokens\": getattr(u, \"candidates_token_count\", None),\n",
    "            \"thoughts_tokens\": getattr(u, \"thoughts_token_count\", None),\n",
    "            \"total_tokens\": getattr(u, \"total_token_count\", None),\n",
    "        }\n",
    "\n",
    "    meta = {\n",
    "        \"model\": model,\n",
    "        \"saved_to\": saved_path,\n",
    "        \"usage\": usage,\n",
    "        \"rows_in\": len(rows),\n",
    "        \"rows_after_clamp\": len(rows_compact),\n",
    "    }\n",
    "    return text, meta\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Invocación\n",
    "# =========================\n",
    "texto, meta = run_analysis(CONFIG)\n",
    "print(texto)\n",
    "meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274c3064",
   "metadata": {},
   "source": [
    "### Aclaración y pasos siguientes\n",
    "\n",
    "Cabe aclarar, que en esta primera instancia de proyecto se busca implementar las etapas 1 y 2. La etapa 3 se integrará luego en el proyecto final. Por tal motivo, también queda pendiente la utlizacion del modelo de texto a imagen.\n",
    "\n",
    "**Optimización:** Ante la gran cantidad de tokens utilizados, luego de comprobar que el codigo funcione, se buscará una manera de optimizarlo para asi lograr una mejor versión final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a9c0d3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
